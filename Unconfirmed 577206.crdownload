# -*- coding: utf-8 -*-
"""GEN_AI_08.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C5YHHs5SYRBNGVY-DjFAe87_x0iyethq
"""

!pip install faiss-cpu sentence-transformers transformers torch

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import json

structured_docs = [
    {"id": 1, "name": "John Doe", "age": 30, "job": "Engineer"},
    {"id": 2, "name": "Jane Smith", "age": 25, "job": "Data Scientist"},
]

unstructured_docs = [
    "John Doe is an engineer with 10 years of experience.",
    "Jane Smith works as a data scientist and loves machine learning.",
]

def structured_to_text(doc):
    return json.dumps(doc)

all_docs_text = [structured_to_text(d) for d in structured_docs] + unstructured_docs

embedder = SentenceTransformer('all-MiniLM-L6-v2')
doc_embeddings = embedder.encode(all_docs_text, convert_to_numpy=True)

dimension = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(doc_embeddings)

model_name = "t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
generator = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def retrieve(query, k=3):
    query_embedding = embedder.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, k)
    return [all_docs_text[i] for i in indices[0]]

def generate_answer(query, retrieved_docs):
    context = " ".join(retrieved_docs)
    input_text = f"question: {query} context: {context}"

    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
    outputs = generator.generate(**inputs, max_length=100)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

!pip install faiss-cpu

query = "Who is Jane Smith and what does she do?"


retrieved = retrieve(query, k=3)
print("Retrieved docs:", retrieved)

answer = generate_answer(query, retrieved)
print("Generated answer:", answer)